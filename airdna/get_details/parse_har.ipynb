{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "212659ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cd51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_har_by_terms(har_file, localidades, details_types, base_output_dir=\"har_results\"):\n",
    "    \"\"\"\n",
    "    Organize HAR file responses into folders and files based on two series of terms.\n",
    "    Skips entries if no matching second term is found.\n",
    "    \n",
    "    Args:\n",
    "        har_file: Path to .har file\n",
    "        localidades: List of terms for folder-level organization\n",
    "        details_types: List of terms for file-level naming\n",
    "        base_output_dir: Base directory for output\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load HAR file\n",
    "        with open(har_file, 'r', encoding='utf-8') as f:\n",
    "            har_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{har_file}' not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: '{har_file}' is not a valid JSON file.\")\n",
    "        return\n",
    "    \n",
    "    # Create base output directory\n",
    "    Path(base_output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Extract entries from HAR file\n",
    "    entries = har_data.get('log', {}).get('entries', [])\n",
    "    \n",
    "    if not entries:\n",
    "        print(\"No entries found in HAR file.\")\n",
    "        return\n",
    "    \n",
    "    summary = {term: {} for term in localidades}\n",
    "    total_saved = 0\n",
    "    skipped_no_second_term = 0\n",
    "    skipped_not_json = 0\n",
    "    \n",
    "    # First pass: organize by first terms (folders)\n",
    "    for localidad in localidades:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing term: '{localidad}'\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Create folder for this term\n",
    "        folder_path = Path(base_output_dir) / sanitize_filename(localidad)\n",
    "        folder_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Track files for this term\n",
    "        summary[localidad] = {\n",
    "            'folder': str(folder_path),\n",
    "            'files': {},\n",
    "            'count': 0\n",
    "        }\n",
    "        \n",
    "        # Find entries matching the first term\n",
    "        matching_entries = []\n",
    "        for entry in entries:\n",
    "            request = entry.get('request', {})\n",
    "            url = request.get('url', '').lower()\n",
    "            method = request.get('method', '')\n",
    "            \n",
    "            # Check if first term is in URL or request\n",
    "            if localidad.lower() in url and method == 'POST' :\n",
    "                matching_entries.append((entry, url))\n",
    "        \n",
    "        print(f\"Found {len(matching_entries)} entries containing '{localidad}'\")\n",
    "        \n",
    "        # Second pass: for each matching entry, check second terms (file names)\n",
    "        entries_saved_this_term = 0\n",
    "        for entry, url in matching_entries:\n",
    "            request = entry.get('request', {})\n",
    "            response = entry.get('response', {})\n",
    "            content = response.get('content', {})\n",
    "            mime_type = content.get('mimeType', '').lower()\n",
    "            text = content.get('text', '')\n",
    "            \n",
    "            # Only process JSON responses\n",
    "            if not ('application/json' in mime_type or 'json' in mime_type):\n",
    "                skipped_not_json += 1\n",
    "                continue\n",
    "            \n",
    "            # Try to find a matching second term for file naming\n",
    "            file_term = None\n",
    "            for second_term in details_types:\n",
    "                # Search in various places for the second term\n",
    "                url_lower = url.lower()\n",
    "                request_text = json.dumps(request).lower()\n",
    "                response_text = json.dumps(response).lower()\n",
    "                content_text = text.lower() if text else \"\"\n",
    "                \n",
    "                # Search in all relevant parts\n",
    "                if second_term.lower() in url_lower:\n",
    "                    file_term = second_term\n",
    "                    break\n",
    "            \n",
    "            # SKIP if no second term found\n",
    "            if not file_term:\n",
    "                continue  # Skip this entry entirely\n",
    "            \n",
    "            # Sanitize file term for filename\n",
    "            sanitized_file_term = sanitize_filename(file_term)\n",
    "            \n",
    "            # Check if file already exists with this name\n",
    "            file_counter = 1\n",
    "            base_filename = sanitized_file_term\n",
    "            while True:\n",
    "                filename = f\"{base_filename}.json\" if file_counter == 1 else f\"{base_filename}_{file_counter}.json\"\n",
    "                filepath = folder_path / filename\n",
    "                if not filepath.exists():\n",
    "                    break\n",
    "                file_counter += 1\n",
    "            \n",
    "            # Extract JSON content\n",
    "            try:\n",
    "                if text:\n",
    "                    json_content = json.loads(text)\n",
    "                else:\n",
    "                    json_content = {}\n",
    "                \n",
    "                # Add metadata\n",
    "                result = {\n",
    "                    'url': url,\n",
    "                    'method': request.get('method', ''),\n",
    "                    'status': response.get('status', 0),\n",
    "                    'timestamp': entry.get('startedDateTime', ''),\n",
    "                    'folder_term': localidad,\n",
    "                    'file_term': file_term,\n",
    "                    'content': json_content\n",
    "                }\n",
    "                \n",
    "                # Save to file\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(json_content, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                # Update summary\n",
    "                if file_term not in summary[localidad]['files']:\n",
    "                    summary[localidad]['files'][file_term] = 0\n",
    "                summary[localidad]['files'][file_term] += 1\n",
    "                summary[localidad]['count'] += 1\n",
    "                total_saved += 1\n",
    "                entries_saved_this_term += 1\n",
    "                \n",
    "                print(f\"  ✓ Saved: {filename} (matched: '{file_term}')\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"  ✗ Could not parse JSON from: {url[:80]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error saving file: {str(e)}\")\n",
    "        \n",
    "        if entries_saved_this_term == 0:\n",
    "            print(f\"  No entries saved for '{localidad}' (no matching second terms found)\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ORGANIZATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total entries processed: {len(entries)}\")\n",
    "    print(f\"Total JSON responses saved: {total_saved}\")\n",
    "    print(f\"Skipped (no second term): {skipped_no_second_term}\")\n",
    "    print(f\"Skipped (not JSON): {skipped_not_json}\")\n",
    "    print(f\"Base output directory: {os.path.abspath(base_output_dir)}\")\n",
    "    print()\n",
    "    \n",
    "    # Only show folders that have saved files\n",
    "    for localidad in localidades:\n",
    "        if summary[localidad]['count'] > 0:\n",
    "            print(f\"Folder: '{localidad}' ({summary[localidad]['count']} files)\")\n",
    "            print(f\"  Path: {summary[localidad]['folder']}\")\n",
    "            \n",
    "            if summary[localidad]['files']:\n",
    "                print(\"  Files organized by second term:\")\n",
    "                for file_term, count in summary[localidad]['files'].items():\n",
    "                    print(f\"    - '{file_term}': {count} file(s)\")\n",
    "            print()\n",
    "    \n",
    "    # Print empty folders warning\n",
    "    empty_folders = [term for term in localidades if summary[term]['count'] == 0]\n",
    "    if empty_folders:\n",
    "        print(f\"Empty folders (no matches with second terms): {', '.join(empty_folders)}\")\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Convert a string to a safe filename\"\"\"\n",
    "    # Replace problematic characters\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
    "    # Remove control characters\n",
    "    filename = re.sub(r'[\\x00-\\x1f\\x7f]', '', filename)\n",
    "    # Limit length\n",
    "    filename = filename[:100]\n",
    "    # Remove leading/trailing dots and spaces\n",
    "    filename = filename.strip('. ')\n",
    "    # If empty after sanitization, use a default\n",
    "    if not filename:\n",
    "        filename = \"file\"\n",
    "    return filename\n",
    "\n",
    "# Alternative version with more options\n",
    "\n",
    "def save_har_entry(entry, localidad, file_term, folder_path, stats):\n",
    "    \"\"\"Save a single HAR entry to file\"\"\"\n",
    "    request = entry.get('request', {})\n",
    "    response = entry.get('response', {})\n",
    "    url = request.get('url', '')\n",
    "    content = response.get('content', {})\n",
    "    text = content.get('text', '')\n",
    "    \n",
    "    # Create filename\n",
    "    base_name = sanitize_filename(file_term)\n",
    "    counter = 1\n",
    "    while True:\n",
    "        filename = f\"{base_name}.json\" if counter == 1 else f\"{base_name}_{counter}.json\"\n",
    "        filepath = folder_path / filename\n",
    "        if not filepath.exists():\n",
    "            break\n",
    "        counter += 1\n",
    "    \n",
    "    # Prepare data\n",
    "    try:\n",
    "        json_content = json.loads(text) if text else {}\n",
    "    except:\n",
    "        json_content = text\n",
    "    \n",
    "    data = {\n",
    "        'metadata': {\n",
    "            'url': url,\n",
    "            'method': request.get('method'),\n",
    "            'status': response.get('status'),\n",
    "            'timestamp': entry.get('startedDateTime'),\n",
    "            'folder_term': localidad,\n",
    "            'file_term': file_term\n",
    "        },\n",
    "        'content': json_content\n",
    "    }\n",
    "    \n",
    "    # Save file\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Update stats\n",
    "        if file_term not in stats['by_folder'][localidad]['files']:\n",
    "            stats['by_folder'][localidad]['files'][file_term] = 0\n",
    "        stats['by_folder'][localidad]['files'][file_term] += 1\n",
    "        \n",
    "        print(f\"  ✓ {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error saving {filename}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def extract_fallback_name(url, request, response):\n",
    "    \"\"\"Extract a fallback name when no second term is found\"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "    path_parts = [p for p in parsed.path.split('/') if p]\n",
    "    \n",
    "    if path_parts:\n",
    "        last_part = path_parts[-1]\n",
    "        # Remove query string and fragments\n",
    "        last_part = last_part.split('?')[0].split('#')[0]\n",
    "        # Remove file extension\n",
    "        last_part = re.sub(r'\\.[a-zA-Z0-9]+$', '', last_part)\n",
    "        if last_part:\n",
    "            return last_part\n",
    "    \n",
    "    # Use method + status\n",
    "    method = request.get('method', 'unknown').lower()\n",
    "    status = response.get('status', 0)\n",
    "    return f\"{method}_{status}\"\n",
    "\n",
    "def print_summary(stats, base_output_dir):\n",
    "    \"\"\"Print organization summary\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total entries processed: {stats['total_processed']}\")\n",
    "    print(f\"Saved: {stats['saved']}\")\n",
    "    print(f\"Skipped - no second term: {stats['skipped_no_second_term']}\")\n",
    "    print(f\"Skipped - not JSON: {stats['skipped_not_json']}\")\n",
    "    print(f\"Output directory: {os.path.abspath(base_output_dir)}\")\n",
    "    \n",
    "    print(\"\\nBy folder:\")\n",
    "    for folder, data in stats['by_folder'].items():\n",
    "        if data['count'] > 0:\n",
    "            print(f\"  {folder}: {data['count']} files\")\n",
    "            for file_term, count in data['files'].items():\n",
    "                print(f\"    {file_term}: {count}\")\n",
    "\n",
    "# def main():\n",
    "#     # Example configuration\n",
    "#     har_file = \"../sources/har/airdna.har\"\n",
    "    \n",
    "#     # First series of terms - creates folders\n",
    "#     localidades = [\"142649\",\"142651\"]\n",
    "    \n",
    "#     # Second series of terms - names files (must be found to save entry)\n",
    "#     details_types = [\"listing_type\",\"bedrooms\",\"minimum_stay\"]\n",
    "    \n",
    "#     # Run organization\n",
    "#     print(\"Organizing HAR file (skipping entries without second term)...\")\n",
    "#     organize_har_by_terms(\n",
    "#         har_file=har_file,\n",
    "#         localidades=localidades,\n",
    "#         details_types=details_types,\n",
    "#         base_output_dir=\"har_organized\"\n",
    "#     )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Command line usage\n",
    "#     import sys\n",
    "#     if len(sys.argv) > 1:\n",
    "#         har_file = sys.argv[1]\n",
    "        \n",
    "#         # Parse terms from command line\n",
    "#         if len(sys.argv) > 2:\n",
    "#             localidades = [t.strip() for t in sys.argv[2].split(',')]\n",
    "#         else:\n",
    "#             localidades = [\"api\", \"user\"]\n",
    "            \n",
    "#         if len(sys.argv) > 3:\n",
    "#             details_types = [t.strip() for t in sys.argv[3].split(',')]\n",
    "#         else:\n",
    "#             details_types = [\"profile\", \"settings\"]\n",
    "            \n",
    "#         base_output_dir = sys.argv[4] if len(sys.argv) > 4 else \"har_results\"\n",
    "        \n",
    "#         organize_har_by_terms(har_file, localidades, details_types, base_output_dir)\n",
    "#     else:\n",
    "#         main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334a1b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     142664\n",
       "1     248952\n",
       "2     142650\n",
       "3     142649\n",
       "4     142663\n",
       "5     142654\n",
       "6     142660\n",
       "7     142658\n",
       "8     249130\n",
       "9     142656\n",
       "10    141883\n",
       "11    142652\n",
       "12    142661\n",
       "13    141029\n",
       "14    142655\n",
       "15    142659\n",
       "16    142651\n",
       "17    142665\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "har_file = \"../sources/har/airdna.har\"\n",
    "    \n",
    "localidades = pd.read_csv('../sources/localidades.csv')['id']\n",
    "localidades = localidades.astype('str')\n",
    "    \n",
    "    # Second series of terms - names files (must be found to save entry)\n",
    "details_types = [\"listing_type\",\"bedrooms\",\"minimum_stay\"]\n",
    "\n",
    "localidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86afc519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizing HAR file (skipping entries without second term)...\n",
      "\n",
      "==================================================\n",
      "Processing term: '142664'\n",
      "==================================================\n",
      "Found 0 entries containing '142664'\n",
      "  No entries saved for '142664' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '248952'\n",
      "==================================================\n",
      "Found 0 entries containing '248952'\n",
      "  No entries saved for '248952' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142650'\n",
      "==================================================\n",
      "Found 0 entries containing '142650'\n",
      "  No entries saved for '142650' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142649'\n",
      "==================================================\n",
      "Found 0 entries containing '142649'\n",
      "  No entries saved for '142649' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142663'\n",
      "==================================================\n",
      "Found 0 entries containing '142663'\n",
      "  No entries saved for '142663' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142654'\n",
      "==================================================\n",
      "Found 0 entries containing '142654'\n",
      "  No entries saved for '142654' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142660'\n",
      "==================================================\n",
      "Found 0 entries containing '142660'\n",
      "  No entries saved for '142660' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142658'\n",
      "==================================================\n",
      "Found 0 entries containing '142658'\n",
      "  No entries saved for '142658' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '249130'\n",
      "==================================================\n",
      "Found 0 entries containing '249130'\n",
      "  No entries saved for '249130' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142656'\n",
      "==================================================\n",
      "Found 0 entries containing '142656'\n",
      "  No entries saved for '142656' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '141883'\n",
      "==================================================\n",
      "Found 6 entries containing '141883'\n",
      "  ✓ Saved: bedrooms_3.json (matched: 'bedrooms')\n",
      "  ✓ Saved: listing_type_3.json (matched: 'listing_type')\n",
      "  ✓ Saved: minimum_stay_3.json (matched: 'minimum_stay')\n",
      "\n",
      "==================================================\n",
      "Processing term: '142652'\n",
      "==================================================\n",
      "Found 0 entries containing '142652'\n",
      "  No entries saved for '142652' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142661'\n",
      "==================================================\n",
      "Found 0 entries containing '142661'\n",
      "  No entries saved for '142661' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '141029'\n",
      "==================================================\n",
      "Found 0 entries containing '141029'\n",
      "  No entries saved for '141029' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142655'\n",
      "==================================================\n",
      "Found 0 entries containing '142655'\n",
      "  No entries saved for '142655' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142659'\n",
      "==================================================\n",
      "Found 0 entries containing '142659'\n",
      "  No entries saved for '142659' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142651'\n",
      "==================================================\n",
      "Found 0 entries containing '142651'\n",
      "  No entries saved for '142651' (no matching second terms found)\n",
      "\n",
      "==================================================\n",
      "Processing term: '142665'\n",
      "==================================================\n",
      "Found 0 entries containing '142665'\n",
      "  No entries saved for '142665' (no matching second terms found)\n",
      "\n",
      "============================================================\n",
      "ORGANIZATION SUMMARY\n",
      "============================================================\n",
      "Total entries processed: 12\n",
      "Total JSON responses saved: 3\n",
      "Skipped (no second term): 0\n",
      "Skipped (not JSON): 0\n",
      "Base output directory: /home/ana/Documents/BOGOTA/alquileres-temporales-dev/airdna/sources\n",
      "\n",
      "Folder: '141883' (3 files)\n",
      "  Path: ../sources/141883\n",
      "  Files organized by second term:\n",
      "    - 'bedrooms': 1 file(s)\n",
      "    - 'listing_type': 1 file(s)\n",
      "    - 'minimum_stay': 1 file(s)\n",
      "\n",
      "Empty folders (no matches with second terms): 142664, 248952, 142650, 142649, 142663, 142654, 142660, 142658, 249130, 142656, 142652, 142661, 141029, 142655, 142659, 142651, 142665\n"
     ]
    }
   ],
   "source": [
    "print(\"Organizing HAR file (skipping entries without second term)...\")\n",
    "organize_har_by_terms(\n",
    "    har_file='../sources/har/chapinero.har',\n",
    "    localidades=localidades,\n",
    "    details_types=details_types,\n",
    "    base_output_dir=\"../sources\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
